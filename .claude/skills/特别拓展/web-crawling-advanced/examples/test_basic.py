"""
åŸºç¡€åŠŸèƒ½æµ‹è¯• - éªŒè¯Crawleeå®‰è£…å’Œæ ¸å¿ƒåŠŸèƒ½

è¿™æ˜¯ä¸€ä¸ªç®€å•çš„æµ‹è¯•è„šæœ¬ï¼Œç”¨äºéªŒè¯Crawleeæ˜¯å¦æ­£ç¡®å®‰è£…å’Œé…ç½®
"""

import asyncio
from pathlib import Path


async def test_import():
    """æµ‹è¯•1: éªŒè¯æ¨¡å—å¯¼å…¥"""
    print("=" * 60)
    print("æµ‹è¯•1: éªŒè¯æ¨¡å—å¯¼å…¥")
    print("=" * 60)

    try:
        from crawlee.beautifulsoup_crawler import BeautifulSoupCrawler, BeautifulSoupCrawlingContext
        from crawlee.playwright_crawler import PlaywrightCrawler, PlaywrightCrawlingContext
        from crawlee.storages import Dataset

        print("âœ… Crawleeæ ¸å¿ƒæ¨¡å—å¯¼å…¥æˆåŠŸ")
        print("  - BeautifulSoupCrawler")
        print("  - PlaywrightCrawler")
        print("  - Dataset")

        return True

    except ImportError as e:
        print(f"âŒ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        print("\nè§£å†³æ–¹æ¡ˆ:")
        print("  pip3 install 'crawlee[all]'")
        print("  playwright install chromium")
        return False


async def test_beautifulsoup_crawler():
    """æµ‹è¯•2: BeautifulSoupçˆ¬è™«"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•2: BeautifulSoupçˆ¬è™« (å¿«é€Ÿé™æ€çˆ¬å–)")
    print("=" * 60)

    try:
        from crawlee.beautifulsoup_crawler import BeautifulSoupCrawler, BeautifulSoupCrawlingContext

        crawler = BeautifulSoupCrawler(max_requests_per_crawl=1)

        results = []

        @crawler.router.default_handler
        async def handler(context: BeautifulSoupCrawlingContext):
            soup = context.soup
            title = soup.find('title')

            data = {
                'url': context.request.url,
                'title': title.get_text() if title else 'No title',
                'status': 'success'
            }

            results.append(data)
            await context.push_data(data)

        # çˆ¬å–ç¤ºä¾‹ç½‘ç«™
        await crawler.run(['https://example.com'])

        if results:
            print(f"âœ… BeautifulSoupçˆ¬è™«æµ‹è¯•æˆåŠŸ")
            print(f"  - URL: {results[0]['url']}")
            print(f"  - æ ‡é¢˜: {results[0]['title']}")
            return True
        else:
            print("âš ï¸ çˆ¬å–æˆåŠŸä½†æœªè·å–åˆ°æ•°æ®")
            return False

    except Exception as e:
        print(f"âŒ BeautifulSoupçˆ¬è™«æµ‹è¯•å¤±è´¥: {e}")
        return False


async def test_playwright_crawler():
    """æµ‹è¯•3: Playwrightçˆ¬è™« (éœ€è¦æµè§ˆå™¨é©±åŠ¨)"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•3: Playwrightçˆ¬è™« (åŠ¨æ€ç½‘é¡µçˆ¬å–)")
    print("=" * 60)

    try:
        from crawlee.playwright_crawler import PlaywrightCrawler, PlaywrightCrawlingContext

        crawler = PlaywrightCrawler(
            headless=True,
            max_requests_per_crawl=1
        )

        results = []

        @crawler.router.default_handler
        async def handler(context: PlaywrightCrawlingContext):
            page = context.page

            data = {
                'url': context.request.url,
                'title': await page.title(),
                'status': 'success'
            }

            results.append(data)
            await context.push_data(data)

        # çˆ¬å–ç¤ºä¾‹ç½‘ç«™
        await crawler.run(['https://example.com'])

        if results:
            print(f"âœ… Playwrightçˆ¬è™«æµ‹è¯•æˆåŠŸ")
            print(f"  - URL: {results[0]['url']}")
            print(f"  - æ ‡é¢˜: {results[0]['title']}")
            return True
        else:
            print("âš ï¸ çˆ¬å–æˆåŠŸä½†æœªè·å–åˆ°æ•°æ®")
            return False

    except Exception as e:
        print(f"âŒ Playwrightçˆ¬è™«æµ‹è¯•å¤±è´¥: {e}")
        print("\nå¯èƒ½åŸå› :")
        print("  1. æµè§ˆå™¨é©±åŠ¨æœªå®‰è£…: playwright install chromium")
        print("  2. æƒé™é—®é¢˜: æ£€æŸ¥æ–‡ä»¶ç³»ç»Ÿæƒé™")
        return False


async def test_data_export():
    """æµ‹è¯•4: æ•°æ®å¯¼å‡ºåŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•4: æ•°æ®å¯¼å‡ºåŠŸèƒ½")
    print("=" * 60)

    try:
        from crawlee.storages import Dataset

        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_data = [
            {'name': 'æµ‹è¯•1', 'value': 100},
            {'name': 'æµ‹è¯•2', 'value': 200},
        ]

        dataset = await Dataset.open()
        await dataset.push_data(test_data)

        # å¯¼å‡ºä¸ºJSON
        output_dir = Path('output/æƒ…æŠ¥ç»„/test-export')
        output_dir.mkdir(parents=True, exist_ok=True)

        await dataset.export_to('output/æƒ…æŠ¥ç»„/test-export/test-data.json')

        print("âœ… æ•°æ®å¯¼å‡ºæµ‹è¯•æˆåŠŸ")
        print(f"  - å¯¼å‡ºè·¯å¾„: output/æƒ…æŠ¥ç»„/test-export/test-data.json")
        print(f"  - æ•°æ®æ¡æ•°: {len(test_data)}")

        return True

    except Exception as e:
        print(f"âŒ æ•°æ®å¯¼å‡ºæµ‹è¯•å¤±è´¥: {e}")
        return False


async def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\nğŸ§ª CrawleeåŠŸèƒ½æµ‹è¯•å¥—ä»¶\n")

    results = {
        'æ¨¡å—å¯¼å…¥': await test_import(),
        'BeautifulSoupçˆ¬è™«': await test_beautifulsoup_crawler(),
        'Playwrightçˆ¬è™«': await test_playwright_crawler(),
        'æ•°æ®å¯¼å‡º': await test_data_export(),
    }

    # ç»Ÿè®¡ç»“æœ
    print("\n" + "=" * 60)
    print("æµ‹è¯•ç»“æœæ±‡æ€»")
    print("=" * 60)

    passed = sum(1 for v in results.values() if v)
    total = len(results)

    for test_name, passed_flag in results.items():
        status = "âœ… é€šè¿‡" if passed_flag else "âŒ å¤±è´¥"
        print(f"{test_name:20} {status}")

    print("=" * 60)
    print(f"æ€»è®¡: {passed}/{total} æµ‹è¯•é€šè¿‡")
    print("=" * 60)

    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Crawleeå·²æ­£ç¡®å®‰è£…å’Œé…ç½®ã€‚")
    else:
        print(f"\nâš ï¸ æœ‰ {total - passed} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®‰è£…å’Œé…ç½®ã€‚")
        print("\nå‚è€ƒæ–‡æ¡£: .claude/skills/web-crawling-advanced/INSTALL.md")

    return passed == total


if __name__ == '__main__':
    asyncio.run(main())
