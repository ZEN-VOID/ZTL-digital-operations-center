# Web Crawling Advanced - æ‰©å±•å‚è€ƒæ–‡æ¡£

> è¯¦ç»†çš„APIæ–‡æ¡£ã€é…ç½®é€‰é¡¹å’Œé«˜çº§ç”¨æ³•

---

## ç›®å½•

1. [APIå®Œæ•´å‚è€ƒ](#apiå®Œæ•´å‚è€ƒ)
2. [é«˜çº§é…ç½®](#é«˜çº§é…ç½®)
3. [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
4. [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)
5. [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)

---

## APIå®Œæ•´å‚è€ƒ

### ZTLCrawler ç±»

#### æ„é€ å‡½æ•°

```python
ZTLCrawler(
    crawler_type: Literal['beautifulsoup', 'playwright', 'adaptive'] = 'beautifulsoup',
    headless: bool = True,
    max_requests: int = 100,
    proxy_config: Optional[Dict] = None,
    retry_config: Optional[Dict] = None
)
```

**å‚æ•°è¯´æ˜**:

- `crawler_type`: çˆ¬è™«å¼•æ“ç±»å‹
  - `'beautifulsoup'`: HTTP + BeautifulSoupï¼ˆå¿«é€Ÿï¼Œé€‚åˆé™æ€é¡µé¢ï¼‰
  - `'playwright'`: å®Œæ•´æµè§ˆå™¨è‡ªåŠ¨åŒ–ï¼ˆåŠŸèƒ½å®Œæ•´ï¼Œé€‚åˆåŠ¨æ€é¡µé¢ï¼‰
  - `'adaptive'`: æ™ºèƒ½è‡ªé€‚åº”ï¼ˆæ ¹æ®é¡µé¢ç±»å‹è‡ªåŠ¨é€‰æ‹©ï¼‰

- `headless`: æ— å¤´æ¨¡å¼ï¼ˆä»…Playwrightï¼‰
  - `True`: åå°è¿è¡Œï¼Œä¸æ˜¾ç¤ºæµè§ˆå™¨ç•Œé¢
  - `False`: æ˜¾ç¤ºæµè§ˆå™¨ç•Œé¢ï¼Œä¾¿äºè°ƒè¯•

- `max_requests`: æœ€å¤§è¯·æ±‚æ•°é™åˆ¶
  - é˜²æ­¢çˆ¬å–è¿‡å¤šé¡µé¢å¯¼è‡´èµ„æºè€—å°½
  - å»ºè®®æ ¹æ®ç›®æ ‡ç½‘ç«™è§„æ¨¡è®¾ç½®ï¼ˆ10-1000ï¼‰

- `proxy_config`: ä»£ç†é…ç½®ï¼ˆå¯é€‰ï¼‰
  ```python
  proxy_config = {
      'server': 'http://proxy.example.com:8080',
      'username': 'user',
      'password': 'pass'
  }
  ```

- `retry_config`: é‡è¯•é…ç½®ï¼ˆå¯é€‰ï¼‰
  ```python
  retry_config = {
      'max_retries': 3,
      'retry_delay': 1000  # æ¯«ç§’
  }
  ```

#### æ ¸å¿ƒæ–¹æ³•

##### crawl()

```python
async def crawl(
    urls: List[str],
    handler: Callable,
    enqueue_links: bool = False
) -> List[Dict[str, Any]]
```

æ‰§è¡Œçˆ¬å–ä»»åŠ¡ã€‚

**å‚æ•°**:
- `urls`: èµ·å§‹URLåˆ—è¡¨
- `handler`: æ•°æ®å¤„ç†å›è°ƒå‡½æ•°
- `enqueue_links`: æ˜¯å¦è‡ªåŠ¨å‘ç°å’Œçˆ¬å–é¡µé¢ä¸­çš„é“¾æ¥

**è¿”å›**:
- é‡‡é›†ç»“æœåˆ—è¡¨

**Handlerå‡½æ•°ç­¾å**:

BeautifulSoupæ¨¡å¼:
```python
async def handler(context: BeautifulSoupCrawlingContext) -> Dict:
    # context.request: è¯·æ±‚å¯¹è±¡
    # context.soup: BeautifulSoupå¯¹è±¡
    # context.http_response: HTTPå“åº”
    return {'key': 'value'}
```

Playwrightæ¨¡å¼:
```python
async def handler(context: PlaywrightCrawlingContext) -> Dict:
    # context.request: è¯·æ±‚å¯¹è±¡
    # context.page: Playwright Pageå¯¹è±¡
    # context.browser_type: æµè§ˆå™¨ç±»å‹
    return {'key': 'value'}
```

##### export_data()

```python
async def export_data(
    output_path: str,
    format: Literal['json', 'csv'] = 'json'
) -> str
```

å¯¼å‡ºé‡‡é›†æ•°æ®åˆ°æ–‡ä»¶ã€‚

**å‚æ•°**:
- `output_path`: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆè‡ªåŠ¨åˆ›å»ºç›®å½•ï¼‰
- `format`: å¯¼å‡ºæ ¼å¼ï¼ˆ`'json'` æˆ– `'csv'`ï¼‰

**è¿”å›**:
- å¯¼å‡ºæ–‡ä»¶çš„ç»å¯¹è·¯å¾„

##### get_stats()

```python
def get_stats() -> Dict[str, Any]
```

è·å–çˆ¬å–ç»Ÿè®¡ä¿¡æ¯ã€‚

**è¿”å›ç¤ºä¾‹**:
```python
{
    'total_requests': 50,
    'successful_requests': 48,
    'failed_requests': 2,
    'start_time': datetime(...),
    'end_time': datetime(...),
    'duration_seconds': 125.5,
    'requests_per_second': 0.398
}
```

---

### ä¾¿æ·å‡½æ•°

#### crawl_competitor_menu()

```python
async def crawl_competitor_menu(
    restaurant_url: str,
    platform: str = 'meituan'
) -> List[Dict]
```

å¿«é€Ÿé‡‡é›†ç«å“èœå•æ•°æ®ã€‚

**å‚æ•°**:
- `restaurant_url`: é¤å…è¯¦æƒ…é¡µURL
- `platform`: å¹³å°ç±»å‹ï¼ˆ`'meituan'` æˆ– `'dianping'`ï¼‰

**è¿”å›ç¤ºä¾‹**:
```python
[{
    'url': 'https://...',
    'platform': 'meituan',
    'restaurant': 'æµ·åº•æç«é”…',
    'dishes': [
        {'name': 'æ¯›è‚š', 'price': '38å…ƒ'},
        {'name': 'é¸­è¡€', 'price': '15å…ƒ'},
        ...
    ],
    'crawled_at': '2025-10-21T10:30:00'
}]
```

#### crawl_reviews()

```python
async def crawl_reviews(
    restaurant_urls: List[str],
    max_reviews_per_restaurant: int = 50
) -> List[Dict]
```

æ‰¹é‡é‡‡é›†é¤å…è¯„ä»·ã€‚

**å‚æ•°**:
- `restaurant_urls`: é¤å…URLåˆ—è¡¨
- `max_reviews_per_restaurant`: æ¯å®¶é¤å…æœ€å¤šé‡‡é›†è¯„ä»·æ•°

**è¿”å›ç¤ºä¾‹**:
```python
[{
    'url': 'https://...',
    'restaurant': 'è¥¿è´èœé¢æ‘',
    'reviews': [
        {
            'user': 'å¼ ä¸‰',
            'rating': '5æ˜Ÿ',
            'content': 'å‘³é“å¾ˆå¥½...'
        },
        ...
    ],
    'crawled_at': '2025-10-21T10:30:00'
}]
```

#### quick_crawl()

```python
async def quick_crawl(
    urls: List[str],
    selector: str,
    output_path: Optional[str] = None
) -> List[Dict]
```

å¿«é€Ÿé‡‡é›†æŒ‡å®šCSSé€‰æ‹©å™¨çš„å†…å®¹ã€‚

**å‚æ•°**:
- `urls`: URLåˆ—è¡¨
- `selector`: CSSé€‰æ‹©å™¨ï¼ˆå¦‚ `'h2'`, `'.title'`ï¼‰
- `output_path`: å¯é€‰çš„è¾“å‡ºè·¯å¾„

---

## é«˜çº§é…ç½®

### ä»£ç†é…ç½®

#### å•ä¸€ä»£ç†

```python
crawler = ZTLCrawler(
    crawler_type='playwright',
    proxy_config={
        'server': 'http://proxy.example.com:8080',
        'username': 'user',
        'password': 'pass'
    }
)
```

#### ä»£ç†è½®æ¢

```python
# éœ€è¦è‡ªå®šä¹‰å®ç°ä»£ç†æ± 
from itertools import cycle

proxy_pool = cycle([
    'http://proxy1.com:8080',
    'http://proxy2.com:8080',
    'http://proxy3.com:8080',
])

# åœ¨handlerä¸­åŠ¨æ€åˆ‡æ¢ä»£ç†
async def handler_with_proxy(context):
    # ä½¿ç”¨next(proxy_pool)è·å–ä¸‹ä¸€ä¸ªä»£ç†
    ...
```

### è¯·æ±‚å¤´è‡ªå®šä¹‰

```python
from crawlee.playwright_crawler import PlaywrightCrawler

crawler = PlaywrightCrawler(
    browser_type='chromium',
    headless=True,
)

# åœ¨handlerä¸­è®¾ç½®è‡ªå®šä¹‰è¯·æ±‚å¤´
async def handler(context):
    await context.page.set_extra_http_headers({
        'User-Agent': 'Custom User Agent',
        'Accept-Language': 'zh-CN,zh;q=0.9',
    })
    ...
```

### Cookieç®¡ç†

```python
async def handler_with_cookie(context: PlaywrightCrawlingContext):
    # è®¾ç½®Cookie
    await context.page.context.add_cookies([
        {
            'name': 'session_id',
            'value': 'abc123',
            'domain': '.example.com',
            'path': '/'
        }
    ])

    # è·å–Cookie
    cookies = await context.page.context.cookies()
    print(f"å½“å‰Cookie: {cookies}")
```

---

## æ€§èƒ½ä¼˜åŒ–

### å¹¶å‘æ§åˆ¶

Crawleeä¼šæ ¹æ®ç³»ç»Ÿèµ„æºè‡ªåŠ¨è°ƒæ•´å¹¶å‘æ•°ï¼Œä½†å¯ä»¥æ‰‹åŠ¨é…ç½®:

```python
from crawlee.playwright_crawler import PlaywrightCrawler

crawler = PlaywrightCrawler(
    max_request_retries=3,
    max_requests_per_crawl=100,
    # å¹¶å‘ç›¸å…³é…ç½®
)
```

### å†…å­˜ä¼˜åŒ–

```python
# 1. é™åˆ¶æœ€å¤§è¯·æ±‚æ•°
crawler = ZTLCrawler(max_requests=50)

# 2. åˆ†æ‰¹å¤„ç†å¤§é‡URL
async def batch_crawl(all_urls, batch_size=50):
    results = []
    for i in range(0, len(all_urls), batch_size):
        batch = all_urls[i:i+batch_size]
        crawler = ZTLCrawler(max_requests=batch_size)
        batch_results = await crawler.crawl(batch, handler)
        results.extend(batch_results)
    return results
```

### è¯·æ±‚é—´éš”

```python
import asyncio

async def handler_with_delay(context):
    # æ•°æ®æå–é€»è¾‘
    data = {...}

    # æ·»åŠ å»¶è¿Ÿï¼ˆç¤¼è²Œçˆ¬å–ï¼‰
    await asyncio.sleep(1)  # 1ç§’å»¶è¿Ÿ

    return data
```

---

## å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•å¤„ç†éœ€è¦ç™»å½•çš„ç½‘ç«™ï¼Ÿ

**A**: ä½¿ç”¨Playwrightæ¨¡å¼ï¼Œå…ˆç™»å½•å†çˆ¬å–:

```python
async def crawl_with_login():
    from crawlee.playwright_crawler import PlaywrightCrawler

    crawler = PlaywrightCrawler(headless=False)

    first_run = True

    @crawler.router.default_handler
    async def handler(context):
        nonlocal first_run

        if first_run:
            # é¦–æ¬¡è®¿é—®ï¼Œæ‰§è¡Œç™»å½•
            await context.page.fill('#username', 'your_username')
            await context.page.fill('#password', 'your_password')
            await context.page.click('#login-button')
            await context.page.wait_for_selector('.user-profile')
            first_run = False

        # æ­£å¸¸çˆ¬å–é€»è¾‘
        ...

    await crawler.run(['https://example.com'])
```

### Q2: å¦‚ä½•ç»•è¿‡åçˆ¬è™«æ£€æµ‹ï¼Ÿ

**A**: å¤šç§ç­–ç•¥ç»„åˆ:

1. **ä½¿ç”¨çœŸå®æµè§ˆå™¨æŒ‡çº¹**ï¼ˆCrawleeé»˜è®¤æ”¯æŒï¼‰
2. **éšæœºUser-Agent**
3. **æ·»åŠ è¯·æ±‚å»¶è¿Ÿ**
4. **ä½¿ç”¨ä»£ç†IPæ± **
5. **æ¨¡æ‹Ÿäººç±»è¡Œä¸º**ï¼ˆéšæœºæ»šåŠ¨ã€ç§»åŠ¨é¼ æ ‡ï¼‰

```python
import random

async def anti_detection_handler(context):
    page = context.page

    # éšæœºæ»šåŠ¨
    for _ in range(random.randint(2, 5)):
        await page.evaluate(f'window.scrollBy(0, {random.randint(100, 500)})')
        await asyncio.sleep(random.uniform(0.5, 1.5))

    # æ•°æ®æå–
    ...
```

### Q3: å¦‚ä½•å¤„ç†åŠ¨æ€åŠ è½½çš„å†…å®¹ï¼Ÿ

**A**: ä½¿ç”¨Playwrightå¹¶ç­‰å¾…å…ƒç´ å‡ºç°:

```python
async def dynamic_handler(context: PlaywrightCrawlingContext):
    page = context.page

    # æ–¹æ³•1: ç­‰å¾…ç‰¹å®šå…ƒç´ 
    await page.wait_for_selector('.dynamic-content', timeout=10000)

    # æ–¹æ³•2: ç­‰å¾…ç½‘ç»œç©ºé—²
    await page.wait_for_load_state('networkidle')

    # æ–¹æ³•3: ç­‰å¾…ç‰¹å®šæ—¶é—´
    await asyncio.sleep(2)

    # æå–æ•°æ®
    ...
```

### Q4: å¦‚ä½•å¯¼å‡ºä¸ºExcelæ ¼å¼ï¼Ÿ

**A**: å…ˆå¯¼å‡ºJSONï¼Œå†è½¬æ¢ä¸ºExcel:

```python
import pandas as pd

# 1. çˆ¬å–æ•°æ®
results = await crawler.crawl(urls, handler)
await crawler.export_data('data.json', format='json')

# 2. è½¬æ¢ä¸ºExcel
df = pd.DataFrame(results)
df.to_excel('data.xlsx', index=False)
```

---

## æœ€ä½³å®è·µ

### 1. éµå®ˆrobots.txt

```python
from urllib.robotparser import RobotFileParser

def can_fetch(url):
    rp = RobotFileParser()
    rp.set_url(f"{url.scheme}://{url.netloc}/robots.txt")
    rp.read()
    return rp.can_fetch("*", url.geturl())

# åœ¨çˆ¬å–å‰æ£€æŸ¥
if can_fetch(target_url):
    await crawler.crawl([target_url], handler)
```

### 2. é”™è¯¯å¤„ç†

```python
async def robust_handler(context):
    try:
        # æ•°æ®æå–é€»è¾‘
        data = extract_data(context)
        return data

    except Exception as e:
        # è®°å½•é”™è¯¯
        print(f"âŒ é”™è¯¯: {e}")

        # è¿”å›é”™è¯¯ä¿¡æ¯
        return {
            'url': context.request.url,
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }
```

### 3. å¢é‡çˆ¬å–

```python
import json
from pathlib import Path

def load_crawled_urls():
    """åŠ è½½å·²çˆ¬å–çš„URL"""
    cache_file = Path('crawled_urls.json')
    if cache_file.exists():
        with open(cache_file) as f:
            return set(json.load(f))
    return set()

def save_crawled_url(url):
    """ä¿å­˜å·²çˆ¬å–çš„URL"""
    crawled = load_crawled_urls()
    crawled.add(url)
    with open('crawled_urls.json', 'w') as f:
        json.dump(list(crawled), f)

async def incremental_handler(context):
    url = context.request.url

    # æ£€æŸ¥æ˜¯å¦å·²çˆ¬å–
    if url in load_crawled_urls():
        print(f"â­ï¸ è·³è¿‡å·²çˆ¬å–URL: {url}")
        return None

    # çˆ¬å–æ•°æ®
    data = extract_data(context)

    # æ ‡è®°ä¸ºå·²çˆ¬å–
    save_crawled_url(url)

    return data
```

### 4. æ•°æ®éªŒè¯

```python
from pydantic import BaseModel, HttpUrl, ValidationError

class DishData(BaseModel):
    name: str
    price: str
    url: HttpUrl

async def validated_handler(context):
    raw_data = extract_data(context)

    try:
        # éªŒè¯æ•°æ®æ ¼å¼
        validated = DishData(**raw_data)
        return validated.dict()

    except ValidationError as e:
        print(f"âŒ æ•°æ®éªŒè¯å¤±è´¥: {e}")
        return None
```

### 5. æ—¥å¿—è®°å½•

```python
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('crawler.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

async def logged_handler(context):
    url = context.request.url
    logger.info(f"ğŸ•·ï¸ å¼€å§‹çˆ¬å–: {url}")

    try:
        data = extract_data(context)
        logger.info(f"âœ… çˆ¬å–æˆåŠŸ: {url}")
        return data

    except Exception as e:
        logger.error(f"âŒ çˆ¬å–å¤±è´¥: {url} - {e}")
        raise
```

---

## ç›¸å…³èµ„æº

- **Crawleeå®˜æ–¹æ–‡æ¡£**: https://crawlee.dev/python/
- **Playwrightæ–‡æ¡£**: https://playwright.dev/python/
- **BeautifulSoupæ–‡æ¡£**: https://www.crummy.com/software/BeautifulSoup/

---

**ç‰ˆæœ¬**: v1.0.0
**æœ€åæ›´æ–°**: 2025-10-21
