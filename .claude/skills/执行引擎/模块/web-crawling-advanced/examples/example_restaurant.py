"""
é¤é¥®è¡Œä¸šåº”ç”¨ç¤ºä¾‹

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨Crawleeé‡‡é›†é¤é¥®ç›¸å…³æ•°æ®
"""

import asyncio
from pathlib import Path
import sys

sys.path.append(str(Path(__file__).parent.parent.parent.parent))

from skills.web_crawling_advanced.scripts.crawlee_engine import (
    ZTLCrawler,
    crawl_competitor_menu,
    crawl_reviews
)


async def example_competitor_analysis():
    """ç¤ºä¾‹: ç«å“åˆ†æ - é‡‡é›†ç«å“èœå•å’Œä»·æ ¼"""

    print("=== ç«å“èœå•é‡‡é›†ç¤ºä¾‹ ===\n")

    # æ³¨æ„: è¿™é‡Œä½¿ç”¨ç¤ºä¾‹URLï¼Œå®é™…ä½¿ç”¨æ—¶æ›¿æ¢ä¸ºçœŸå®URL
    restaurant_url = 'https://www.meituan.com/meishi/example-restaurant'

    try:
        # ä½¿ç”¨ä¾¿æ·å‡½æ•°é‡‡é›†èœå•
        # results = await crawl_competitor_menu(restaurant_url, platform='meituan')

        # print(f"âœ… èœå•é‡‡é›†å®Œæˆ")
        # print(f"   é¤å…: {results[0]['restaurant']}")
        # print(f"   èœå“æ•°: {len(results[0]['dishes'])}")
        # print(f"   ç¤ºä¾‹èœå“: {results[0]['dishes'][:3]}")

        print("âš ï¸ è¿™æ˜¯ç¤ºä¾‹ä»£ç ï¼Œè¯·æ›¿æ¢ä¸ºå®é™…URLåè¿è¡Œ")

    except Exception as e:
        print(f"âŒ é‡‡é›†å¤±è´¥: {e}")


async def example_review_monitoring():
    """ç¤ºä¾‹: è¯„ä»·ç›‘æ§ - æ‰¹é‡é‡‡é›†é¤å…è¯„ä»·"""

    print("\n=== é¤å…è¯„ä»·é‡‡é›†ç¤ºä¾‹ ===\n")

    restaurant_urls = [
        'https://www.dianping.com/shop/example1',
        'https://www.dianping.com/shop/example2',
        # æ·»åŠ æ›´å¤šé¤å…URL...
    ]

    try:
        # results = await crawl_reviews(restaurant_urls, max_reviews_per_restaurant=20)

        # print(f"âœ… è¯„ä»·é‡‡é›†å®Œæˆ")
        # print(f"   é‡‡é›†é¤å…æ•°: {len(results)}")
        # for result in results:
        #     print(f"   - {result['restaurant']}: {len(result['reviews'])} æ¡è¯„ä»·")

        print("âš ï¸ è¿™æ˜¯ç¤ºä¾‹ä»£ç ï¼Œè¯·æ›¿æ¢ä¸ºå®é™…URLåè¿è¡Œ")

    except Exception as e:
        print(f"âŒ é‡‡é›†å¤±è´¥: {e}")


async def example_custom_restaurant_crawler():
    """ç¤ºä¾‹: è‡ªå®šä¹‰é¤å…æ•°æ®çˆ¬è™«"""

    print("\n=== è‡ªå®šä¹‰é¤å…æ•°æ®çˆ¬è™« ===\n")

    from crawlee.playwright_crawler import PlaywrightCrawlingContext

    crawler = ZTLCrawler(crawler_type='playwright', headless=True)

    async def restaurant_handler(context: PlaywrightCrawlingContext):
        """æå–é¤å…åŸºç¡€ä¿¡æ¯"""

        page = context.page

        # ç­‰å¾…é¡µé¢åŠ è½½
        await page.wait_for_load_state('networkidle')

        # æå–æ•°æ®ï¼ˆæ ¹æ®å®é™…ç½‘ç«™ç»“æ„è°ƒæ•´é€‰æ‹©å™¨ï¼‰
        try:
            # é¤å…åç§°
            name_el = await page.query_selector('.restaurant-name')
            name = await name_el.inner_text() if name_el else ''

            # è¯„åˆ†
            rating_el = await page.query_selector('.rating-score')
            rating = await rating_el.inner_text() if rating_el else ''

            # äººå‡æ¶ˆè´¹
            price_el = await page.query_selector('.avg-price')
            avg_price = await price_el.inner_text() if price_el else ''

            # åœ°å€
            address_el = await page.query_selector('.restaurant-address')
            address = await address_el.inner_text() if address_el else ''

            # è¥ä¸šæ—¶é—´
            hours_el = await page.query_selector('.business-hours')
            hours = await hours_el.inner_text() if hours_el else ''

            return {
                'url': context.request.url,
                'name': name,
                'rating': rating,
                'avg_price': avg_price,
                'address': address,
                'business_hours': hours,
            }

        except Exception as e:
            print(f"æå–æ•°æ®å¤±è´¥: {e}")
            return {'url': context.request.url, 'error': str(e)}

    # ç¤ºä¾‹URLåˆ—è¡¨
    urls = [
        'https://example-restaurant-site.com/restaurant/1',
        # æ·»åŠ æ›´å¤šURL...
    ]

    # results = await crawler.crawl(urls, restaurant_handler)
    # await crawler.export_data('output/æƒ…æŠ¥ç»„/restaurant-data.json')

    print("âš ï¸ è¿™æ˜¯ç¤ºä¾‹ä»£ç ï¼Œè¯·æ ¹æ®å®é™…ç½‘ç«™ç»“æ„è°ƒæ•´é€‰æ‹©å™¨")


async def example_price_tracking():
    """ç¤ºä¾‹: ä»·æ ¼è¿½è¸ª - å®šæœŸç›‘æ§ç«å“ä»·æ ¼å˜åŒ–"""

    print("\n=== ä»·æ ¼è¿½è¸ªç¤ºä¾‹ ===\n")

    from crawlee.beautifulsoup_crawler import BeautifulSoupCrawlingContext
    from datetime import datetime

    crawler = ZTLCrawler(crawler_type='beautifulsoup')

    async def price_handler(context: BeautifulSoupCrawlingContext):
        soup = context.soup

        # æå–å•†å“ä»·æ ¼ï¼ˆæ ¹æ®å®é™…ç½‘ç«™è°ƒæ•´ï¼‰
        prices = []
        price_elements = soup.select('.product-price')

        for el in price_elements:
            product_name = el.find_previous('.product-name')
            price_text = el.get_text(strip=True)

            prices.append({
                'product': product_name.get_text() if product_name else '',
                'price': price_text,
                'timestamp': datetime.now().isoformat()
            })

        return {
            'url': context.request.url,
            'prices': prices,
            'crawled_at': datetime.now().isoformat()
        }

    # å®šæœŸé‡‡é›†ä»·æ ¼æ•°æ®
    urls = ['https://example-menu-site.com']
    # results = await crawler.crawl(urls, price_handler)

    # ä¿å­˜å†å²æ•°æ®ç”¨äºå¯¹æ¯”
    # output_file = f"output/æƒ…æŠ¥ç»„/price-tracking-{datetime.now().strftime('%Y%m%d')}.json"
    # await crawler.export_data(output_file)

    print("ğŸ’¡ æç¤º: å¯ä»¥é…åˆcronå®šæ—¶ä»»åŠ¡ï¼Œæ¯å¤©è‡ªåŠ¨é‡‡é›†ä»·æ ¼æ•°æ®")


async def main():
    """è¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""

    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path('output/æƒ…æŠ¥ç»„').mkdir(parents=True, exist_ok=True)

    print("="*60)
    print("é¤é¥®è¡Œä¸šçˆ¬è™«åº”ç”¨ç¤ºä¾‹")
    print("="*60 + "\n")

    # è¿è¡Œç¤ºä¾‹
    await example_competitor_analysis()
    await example_review_monitoring()
    await example_custom_restaurant_crawler()
    await example_price_tracking()

    print("\n" + "="*60)
    print("ğŸ’¡ ä½¿ç”¨æç¤º:")
    print("1. å°†ç¤ºä¾‹URLæ›¿æ¢ä¸ºå®é™…ç½‘ç«™URL")
    print("2. æ ¹æ®ç›®æ ‡ç½‘ç«™è°ƒæ•´CSSé€‰æ‹©å™¨")
    print("3. éµå®ˆrobots.txtå’Œç½‘ç«™æœåŠ¡æ¡æ¬¾")
    print("4. è®¾ç½®åˆç†çš„è¯·æ±‚é—´éš”ï¼Œé¿å…è¿‡è½½")
    print("="*60)


if __name__ == '__main__':
    asyncio.run(main())
