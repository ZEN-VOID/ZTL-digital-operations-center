"""
åŸºç¡€ç¤ºä¾‹ - å¿«é€Ÿä¸Šæ‰‹Crawleeçˆ¬è™«

æ¼”ç¤ºæœ€åŸºç¡€çš„ä½¿ç”¨æ–¹æ³•
"""

import asyncio
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent.parent.parent))

from skills.web_crawling_advanced.scripts.crawlee_engine import ZTLCrawler, quick_crawl


async def example1_quick_crawl():
    """ç¤ºä¾‹1: ä½¿ç”¨ä¾¿æ·å‡½æ•°å¿«é€Ÿé‡‡é›†"""

    print("=== ç¤ºä¾‹1: å¿«é€Ÿé‡‡é›†ç½‘é¡µæ ‡é¢˜ ===\n")

    # ä¸€è¡Œä»£ç é‡‡é›†æ‰€æœ‰h2æ ‡é¢˜
    results = await quick_crawl(
        urls=['https://example.com'],
        selector='h2',
        output_path='output/æƒ…æŠ¥ç»„/example-quick-crawl.json'
    )

    print(f"âœ… é‡‡é›†å®Œæˆ")
    print(f"   URL: {results[0]['url']}")
    print(f"   æ‰¾åˆ° {results[0]['count']} ä¸ªæ ‡é¢˜")
    print(f"   å†…å®¹: {results[0]['content'][:3]}")  # å‰3ä¸ªæ ‡é¢˜


async def example2_custom_static():
    """ç¤ºä¾‹2: è‡ªå®šä¹‰é™æ€ç½‘é¡µçˆ¬è™«"""

    print("\n=== ç¤ºä¾‹2: è‡ªå®šä¹‰é™æ€çˆ¬è™« ===\n")

    from crawlee.beautifulsoup_crawler import BeautifulSoupCrawlingContext

    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = ZTLCrawler(crawler_type='beautifulsoup', max_requests=5)

    # å®šä¹‰æ•°æ®å¤„ç†é€»è¾‘
    async def my_handler(context: BeautifulSoupCrawlingContext):
        soup = context.soup

        # æå–æ ‡é¢˜
        title = soup.find('title')

        # æå–æ‰€æœ‰æ®µè½
        paragraphs = soup.find_all('p')

        return {
            'url': context.request.url,
            'title': title.get_text() if title else '',
            'paragraph_count': len(paragraphs),
            'first_paragraph': paragraphs[0].get_text() if paragraphs else ''
        }

    # æ‰§è¡Œçˆ¬å–
    results = await crawler.crawl(['https://example.com'], my_handler)

    # å¯¼å‡ºç»“æœ
    await crawler.export_data('output/æƒ…æŠ¥ç»„/example-custom-static.json')

    # æ‰“å°ç»“æœ
    print(f"âœ… é‡‡é›†å®Œæˆ")
    print(f"   æ ‡é¢˜: {results[0]['title']}")
    print(f"   æ®µè½æ•°: {results[0]['paragraph_count']}")

    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    stats = crawler.get_stats()
    print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   æ€»è¯·æ±‚: {stats['total_requests']}")
    print(f"   æˆåŠŸ: {stats['successful_requests']}")
    print(f"   å¤±è´¥: {stats['failed_requests']}")


async def example3_multiple_urls():
    """ç¤ºä¾‹3: æ‰¹é‡é‡‡é›†å¤šä¸ªURL"""

    print("\n=== ç¤ºä¾‹3: æ‰¹é‡é‡‡é›† ===\n")

    from crawlee.beautifulsoup_crawler import BeautifulSoupCrawlingContext

    crawler = ZTLCrawler(crawler_type='beautifulsoup', max_requests=10)

    async def handler(context: BeautifulSoupCrawlingContext):
        soup = context.soup
        title = soup.find('title')

        return {
            'url': context.request.url,
            'title': title.get_text() if title else 'No title'
        }

    # æ‰¹é‡URL
    urls = [
        'https://example.com',
        'https://example.org',
        'https://example.net',
    ]

    results = await crawler.crawl(urls, handler)

    print(f"âœ… æ‰¹é‡é‡‡é›†å®Œæˆï¼Œå…± {len(results)} ä¸ªURL")
    for i, result in enumerate(results, 1):
        print(f"   {i}. {result['title']}")

    # å¯¼å‡º
    await crawler.export_data('output/æƒ…æŠ¥ç»„/example-batch-crawl.json')


async def main():
    """è¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""

    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path('output/æƒ…æŠ¥ç»„').mkdir(parents=True, exist_ok=True)

    # è¿è¡Œç¤ºä¾‹
    await example1_quick_crawl()
    await example2_custom_static()
    await example3_multiple_urls()

    print("\n" + "="*50)
    print("âœ… æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆ!")
    print("ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: output/æƒ…æŠ¥ç»„/")
    print("="*50)


if __name__ == '__main__':
    asyncio.run(main())
