"""
æ‰¹é‡ä»è…¾è®¯äº‘COSæå–å›¾ç‰‡URL - ä½¿ç”¨qcloud_cosç›´æ¥å®ç°

æ‰§è¡Œ/2æŒ‡ä»¤çš„æ ¸å¿ƒåŠŸèƒ½:ä»COSç›®å½•æå–æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶çš„URL,
æŒ‰å­ç›®å½•ç»“æ„ä¿å­˜ä¸ºJSONç´¢å¼•æ–‡ä»¶ã€‚
"""

import os
import json
from pathlib import Path
from datetime import datetime, timedelta
from qcloud_cos import CosConfig, CosS3Client
from dotenv import load_dotenv


def create_cos_client():
    """åˆ›å»ºCOSå®¢æˆ·ç«¯"""
    load_dotenv()

    secret_id = os.getenv('SecretId')
    secret_key = os.getenv('SecretKey')
    region = os.getenv('Region', 'ap-chengdu')

    if not secret_id or not secret_key:
        raise ValueError("ç¼ºå°‘SecretIdæˆ–SecretKeyé…ç½®")

    config = CosConfig(
        Region=region,
        SecretId=secret_id,
        SecretKey=secret_key,
        Scheme='https'
    )

    return CosS3Client(config)


def list_all_files(client, bucket, prefix):
    """åˆ—å‡ºCOSç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶(å¸¦åˆ†é¡µå¤„ç†)"""
    print(f"\nğŸ“‚ æ‰«æCOSç›®å½•: {prefix}")

    files = []
    marker = ""

    while True:
        response = client.list_objects(
            Bucket=bucket,
            Prefix=prefix,
            Marker=marker,
            MaxKeys=1000
        )

        if 'Contents' in response:
            for item in response['Contents']:
                # è·³è¿‡ç›®å½•æ ‡è®°
                if item['Key'].endswith('/'):
                    continue
                if item['Key'] == prefix or item['Key'] == prefix.rstrip('/'):
                    continue
                files.append({
                    'key': item['Key'],
                    'size': item['Size'],
                    'last_modified': item['LastModified']
                })

        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šæ•°æ®
        if response.get('IsTruncated') == 'false':
            break
        marker = response.get('NextMarker', '')

    print(f"âœ… æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶")
    return files


def generate_presigned_url(client, bucket, object_key, expires=86400):
    """ç”Ÿæˆé¢„ç­¾åURLï¼ˆé»˜è®¤24å°æ—¶ï¼‰"""
    try:
        url = client.get_presigned_url(
            Method='GET',
            Bucket=bucket,
            Key=object_key,
            Expired=expires
        )
        return url
    except Exception as e:
        print(f"  âŒ ç”ŸæˆURLå¤±è´¥: {object_key} - {str(e)}")
        return None


def extract_urls(client, bucket, cos_prefix, local_base_path, url_expires=86400):
    """æå–URLå¹¶ä¿å­˜ä¸ºJSONç´¢å¼•"""
    print(f"\n{'=' * 80}")
    print(f"ğŸ“¦ ä»»åŠ¡: {cos_prefix} â†’ {local_base_path}")
    print(f"{'=' * 80}")

    # åˆ—å‡ºæ‰€æœ‰æ–‡ä»¶
    all_files = list_all_files(client, bucket, cos_prefix)

    if not all_files:
        print(f"âš ï¸  ç›®å½•ä¸ºç©ºæˆ–ä¸å­˜åœ¨")
        return {'success': False, 'message': 'ç›®å½•ä¸ºç©º'}

    # è¿‡æ»¤å›¾ç‰‡æ–‡ä»¶ï¼ˆæ’é™¤HEICæ ¼å¼ï¼‰
    # âš ï¸ é‡è¦ï¼šHEICæ ¼å¼åœ¨æµè§ˆå™¨ä¸­ä¸è¢«æ”¯æŒï¼Œå¿…é¡»æ’é™¤
    image_extensions = {'.png', '.jpg', '.jpeg', '.webp', '.gif', '.bmp', '.svg', '.PNG', '.JPG', '.JPEG'}
    heic_extensions = {'.heic', '.HEIC'}  # æ˜ç¡®æ’é™¤çš„æ ¼å¼

    image_files = [
        f for f in all_files
        if Path(f['key']).suffix in image_extensions
        and Path(f['key']).suffix not in heic_extensions
    ]

    print(f"ğŸ” è¯†åˆ«å‡º {len(image_files)} ä¸ªå›¾ç‰‡æ–‡ä»¶\n")

    if not image_files:
        print(f"âš ï¸  æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶")
        return {'success': False, 'message': 'æ²¡æœ‰å›¾ç‰‡æ–‡ä»¶'}

    # æŒ‰ç›®å½•ç»„ç»‡æ–‡ä»¶
    dir_files_map = {}

    print(f"ğŸ”— ç”Ÿæˆé¢„ç­¾åURL...")
    for idx, file_info in enumerate(image_files, 1):
        file_key = file_info['key']
        file_path = Path(file_key)

        # è®¡ç®—ç›¸å¯¹äºprefixçš„è·¯å¾„
        relative_path = file_key[len(cos_prefix):].lstrip('/')
        relative_dir = str(Path(relative_path).parent) if '/' in relative_path else ''

        # ç”ŸæˆURL
        url = generate_presigned_url(client, bucket, file_key, url_expires)
        if not url:
            continue

        # è®¡ç®—URLè¿‡æœŸæ—¶é—´
        expires_at = datetime.now() + timedelta(seconds=url_expires)

        file_data = {
            "id": idx,
            "filename": file_path.name,
            "object_key": file_key,
            "size": file_info['size'],
            "last_modified": file_info['last_modified'],
            "url": url,
            "url_valid_until": expires_at.isoformat()
        }

        # æŒ‰ç›®å½•åˆ†ç»„
        if relative_dir not in dir_files_map:
            dir_files_map[relative_dir] = []
        dir_files_map[relative_dir].append(file_data)

        print(f"  [{idx}/{len(image_files)}] {file_path.name}")

    print(f"\nâœ… æˆåŠŸç”Ÿæˆ {sum(len(files) for files in dir_files_map.values())} ä¸ªURL\n")

    # åˆ›å»ºæœ¬åœ°ç›®å½•å¹¶ä¿å­˜JSON
    print(f"ğŸ’¾ ä¿å­˜ç´¢å¼•æ–‡ä»¶...")
    local_base = Path(local_base_path)
    local_base.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    saved_files = []

    for dir_path, files_list in dir_files_map.items():
        # åˆ›å»ºå¯¹åº”çš„æœ¬åœ°ç›®å½•
        if dir_path:
            local_dir = local_base / dir_path
        else:
            local_dir = local_base

        local_dir.mkdir(parents=True, exist_ok=True)

        # ç”ŸæˆJSONç´¢å¼•
        json_data = {
            "metadata": {
                "source": "cos",
                "bucket": bucket,
                "prefix": cos_prefix,
                "directory": dir_path if dir_path else "(root)",
                "scan_time": timestamp,
                "total_images": len(files_list),
                "url_expires": url_expires,
                "version": "1.0"
            },
            "images": files_list
        }

        # ç”Ÿæˆæ–‡ä»¶å(å›ºå®šæ ¼å¼,ä¸å¸¦æ—¶é—´æˆ³)
        dir_safe = dir_path.replace('/', '-') if dir_path else 'root'
        json_filename = f"cos-index-{dir_safe}.json"
        json_path = local_dir / json_filename

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        if json_path.exists():
            print(f"  âš ï¸  æ–‡ä»¶å·²å­˜åœ¨,è¦†ç›–: {json_path}")

        # ä¿å­˜JSON (è¦†ç›–æ¨¡å¼)
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)

        saved_files.append(str(json_path))
        print(f"  âœ… {json_path}")

    # ç»Ÿè®¡ä¿¡æ¯
    total_images = sum(len(files) for files in dir_files_map.values())
    total_size = sum(int(file['size']) if isinstance(file['size'], str) else file['size']
                     for files in dir_files_map.values() for file in files)

    print(f"\n{'=' * 80}")
    print(f"ğŸ‰ ç´¢å¼•ç”Ÿæˆå®Œæˆï¼")
    print(f"{'=' * 80}\n")
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  - æ€»æ–‡ä»¶æ•°: {total_images} ä¸ª")
    print(f"  - æ€»å¤§å°: {total_size / 1024 / 1024:.2f} MB")
    print(f"  - ç›®å½•å±‚çº§: {len(dir_files_map)} ä¸ª")
    print(f"  - ç´¢å¼•æ–‡ä»¶: {len(saved_files)} ä¸ª")
    print(f"  - URLæœ‰æ•ˆæœŸ: {url_expires / 3600:.1f} å°æ—¶")
    print(f"  - è¿‡æœŸæ—¶é—´: {(datetime.now() + timedelta(seconds=url_expires)).strftime('%Y-%m-%d %H:%M:%S')}")
    print()

    return {
        'success': True,
        'total_images': total_images,
        'total_size': total_size,
        'directories': len(dir_files_map),
        'index_files': saved_files
    }


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("ğŸš€ COSæ‰¹é‡URLæå–ä»»åŠ¡ - /2æŒ‡ä»¤")
    print("=" * 80)

    # åˆ›å»ºCOSå®¢æˆ·ç«¯
    print("\nğŸ“‹ åˆå§‹åŒ–COSå®¢æˆ·ç«¯...")
    try:
        client = create_cos_client()
        bucket = os.getenv('Bucket', 'minghong-redbook-1353737511')
        print(f"âœ… å®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
        print(f"   Bucket: {bucket}")
    except Exception as e:
        print(f"âŒ å®¢æˆ·ç«¯åˆ›å»ºå¤±è´¥: {str(e)}")
        return

    # é¡¹ç›®æ ¹ç›®å½•
    project_root = Path(__file__).parent.parent

    # å®šä¹‰ä»»åŠ¡
    tasks = [
        {
            'cos_prefix': 'è¾“å…¥/æ—…æ¸¸/æ¨¡æ¿5/',
            'local_path': project_root / 'input' / 'æ˜çº¢' / 'å›¾ç‰‡URL' / 'æ¨¡æ¿5'
        },
        {
            'cos_prefix': 'è¾“å…¥/æ—…æ¸¸/æ¨¡æ¿6/',
            'local_path': project_root / 'input' / 'æ˜çº¢' / 'å›¾ç‰‡URL' / 'æ¨¡æ¿6'
        },
        {
            'cos_prefix': 'è¾“å…¥/æ—…æ¸¸/æ¨¡æ¿7/',
            'local_path': project_root / 'input' / 'æ˜çº¢' / 'å›¾ç‰‡URL' / 'æ¨¡æ¿7'
        }
    ]

    # æ‰§è¡Œä»»åŠ¡
    all_results = []

    for idx, task in enumerate(tasks, 1):
        print(f"\n\n{'#' * 80}")
        print(f"ä»»åŠ¡ {idx}/{len(tasks)}: {task['cos_prefix']}")
        print(f"{'#' * 80}\n")

        result = extract_urls(
            client=client,
            bucket=bucket,
            cos_prefix=task['cos_prefix'],
            local_base_path=task['local_path'],
            url_expires=86400  # 24å°æ—¶
        )

        all_results.append({
            'task': task,
            'result': result
        })

    # æ‰“å°æ€»ç»“
    print(f"\n{'=' * 80}")
    print("ğŸ“Š å…¨éƒ¨ä»»åŠ¡æ€»ç»“")
    print(f"{'=' * 80}\n")

    total_images = 0
    total_index_files = 0

    for idx, item in enumerate(all_results, 1):
        task = item['task']
        result = item['result']

        print(f"ä»»åŠ¡ {idx}: {task['cos_prefix']}")
        if result.get('success'):
            print(f"  âœ… æˆåŠŸ")
            print(f"  - å›¾ç‰‡æ€»æ•°: {result['total_images']} ä¸ª")
            print(f"  - å­ç›®å½•æ•°: {result['directories']} ä¸ª")
            print(f"  - ç´¢å¼•æ–‡ä»¶: {len(result['index_files'])} ä¸ª")
            total_images += result['total_images']
            total_index_files += len(result['index_files'])
        else:
            print(f"  âŒ å¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
        print()

    print(f"{'=' * 80}")
    print(f"âœ… å…¨éƒ¨å®Œæˆ!")
    print(f"   æ€»å›¾ç‰‡æ•°: {total_images} ä¸ª")
    print(f"   æ€»ç´¢å¼•æ–‡ä»¶: {total_index_files} ä¸ª")
    print(f"{'=' * 80}\n")


if __name__ == '__main__':
    main()
